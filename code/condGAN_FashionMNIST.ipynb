{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo7agWw2UQn5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense,Reshape,Dropout,BatchNormalization,Flatten,Input,Conv2D,Conv2DTranspose,LeakyReLU,Dropout,Embedding,concatenate \n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4RPwtReUcRH"
   },
   "outputs": [],
   "source": [
    "#define parameters\n",
    "LATENT_DIM = 64\n",
    "BATCH_SIZE = 64\n",
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnucNihaUafc",
    "outputId": "f85783be-aa5e-4a4b-d92d-9c7feec9fb95"
   },
   "outputs": [],
   "source": [
    "#get the dataset\n",
    "(x_train,y_train),_ = tf.keras.datasets.fashion_mnist.load_data()\n",
    "#one hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train,CLASSES)\n",
    "#normalize the images\n",
    "x_train = x_train.astype(np.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-fI634sUls9"
   },
   "outputs": [],
   "source": [
    "#convert to tf dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(3000)\n",
    "dataset = dataset.batch(BATCH_SIZE,drop_remainder = True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4Q2RyBiUrrc",
    "outputId": "e71f14d0-dd37-44a4-8692-b7155804bad2"
   },
   "outputs": [],
   "source": [
    "#define the generator model\n",
    "def generator():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(Dense(7*7*74,input_shape=[LATENT_DIM+CLASSES]))\n",
    "  model.add(LeakyReLU(0.1))\n",
    "  \n",
    "  model.add(Reshape([7,7,74]))\n",
    "  model.add(Conv2DTranspose(138,(4,4),padding='same',strides=(2,2)))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  \n",
    "  model.add(Conv2DTranspose(1,(4,4),padding='same',activation=\"sigmoid\",strides=(2,2)))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add((BatchNormalization()))\n",
    "  \n",
    " \n",
    "  \n",
    "  return model\n",
    "\n",
    "gen = generator()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGu0Fp3iVJc2",
    "outputId": "478cfc2b-b5ae-40ec-973c-28fd753bdd12"
   },
   "outputs": [],
   "source": [
    "#define discriminator model\n",
    "def discriminator():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(Conv2D(138,(4,4),strides=(2,2),padding=\"same\",input_shape = [28,28,1+CLASSES]))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  \n",
    "  model.add(Conv2D(74,(4,4),strides=(2,2),padding=\"valid\"))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(74))\n",
    "  model.add(LeakyReLU(0.2))\n",
    "  model.add((BatchNormalization()))\n",
    "  model.add(Dense(1,activation=\"sigmoid\"))\n",
    "  return model\n",
    "\n",
    "dis = discriminator()\n",
    "dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ2KxC7oWZ4r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPNHS9_sbV18"
   },
   "outputs": [],
   "source": [
    "#instantiate\n",
    "#note that defining composite will not be useful here\n",
    "#as we need to take as input the class label\n",
    "dis = discriminator()\n",
    "gen = generator()\n",
    "dis.compile(loss=\"binary_crossentropy\",optimizer=\"Adam\")\n",
    "dis.trainable = False\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "Ss9HJel8dgLd",
    "outputId": "6d4f2d1a-fbb9-4ab7-86d2-84582ccff6a5"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "#some print statements are commented out, were useful for debugging\n",
    "def train(gen,dis,dataset,num_epochs):\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for real_image_batch,label_batch in dataset:\n",
    "      #print(label_batch.shape)\n",
    "      labels = label_batch[:,:,None,None]#reshape labels for discrim input\n",
    "      #print(labels.shape)\n",
    "      labels = tf.repeat(labels,repeats = [28*28])#create image shape of labels\n",
    "      #print(labels.shape)\n",
    "      labels = tf.reshape(labels,(-1,28,28,CLASSES)) #reshape to compatible shape\n",
    "      #print(labels.shape)\n",
    "      latent = tf.random.normal([BATCH_SIZE,LATENT_DIM]) \n",
    "      #print(latent.shape)\n",
    "      latent = tf.concat([latent,label_batch],axis=1)#input to gen\n",
    "      #print(latent.shape)\n",
    "      gen_image_batch = gen(latent)\n",
    "      #print(gen_image_batch.shape)\n",
    "      real_image_and_labels = tf.concat([real_image_batch[:,:,:,None],labels],-1)\n",
    "      fake_image_and_labels = tf.concat([gen_image_batch,labels],-1)\n",
    "      #create input for discrim training\n",
    "      x_concat = tf.concat([real_image_and_labels,fake_image_and_labels],axis=0)\n",
    "      y_concat = tf.concat([tf.convert_to_tensor(np.ones(BATCH_SIZE)),tf.convert_to_tensor(np.zeros(BATCH_SIZE))],axis=0)\n",
    "      dis.trainable = True\n",
    "      dis.train_on_batch(x_concat,y_concat)\n",
    "      #freeze the discriminator\n",
    "      dis.trainable = False\n",
    "      latent = tf.random.normal([BATCH_SIZE,LATENT_DIM])\n",
    "      latent = tf.concat([latent,label_batch],axis=1)\n",
    "      y = tf.convert_to_tensor(np.ones(BATCH_SIZE))\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        gen_image_batch = gen(latent)\n",
    "        fake_image_and_labels = tf.concat([gen_image_batch,labels],-1)\n",
    "        pred = dis(fake_image_and_labels)\n",
    "        gen_loss = loss(y,pred)\n",
    "      grad = tape.gradient(gen_loss,gen.trainable_weights)\n",
    "      #train the generator\n",
    "      opt.apply_gradients(zip(grad, gen.trainable_variables))\n",
    "  plot(gen_image_batch)\n",
    "\n",
    "train(gen,dis,dataset,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "xzj-y9KqhAKh",
    "outputId": "ca8bcbf4-3c7a-439a-c1e8-4d5c91dd72af"
   },
   "outputs": [],
   "source": [
    "#create input to test and plot\n",
    "arr = [[1]*8 + [2]*8 + [3]*8 + [4]*8 + [5]*8 + [6]*8 + [7]*8+ [8]*8]\n",
    "arr = arr[0]\n",
    "arr = tf.keras.utils.to_categorical(tf.convert_to_tensor(arr),CLASSES)\n",
    "print(arr.shape)\n",
    "latent = tf.random.normal([BATCH_SIZE,LATENT_DIM])\n",
    "print(latent.shape)\n",
    "latent = tf.concat([latent,arr],axis=1)\n",
    "print(latent.shape)\n",
    "gen_image_batch = gen(latent)\n",
    "plot(gen_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "C9Sg050bn9Uk",
    "outputId": "4063faf8-078a-460f-ea22-c3ff033ebca3"
   },
   "outputs": [],
   "source": [
    "arr = [[1]*8 + [2]*8 + [3]*8 + [4]*8 + [5]*8 + [6]*8 + [7]*8+ [8]*8]\n",
    "arr = arr[0]\n",
    "arr = tf.keras.utils.to_categorical(tf.convert_to_tensor(arr),CLASSES)\n",
    "print(arr.shape)\n",
    "latent = tf.random.normal([BATCH_SIZE,LATENT_DIM])\n",
    "print(latent.shape)\n",
    "latent = tf.concat([latent,arr],axis=1)\n",
    "print(latent.shape)\n",
    "gen_image_batch = gen(latent)\n",
    "plot(gen_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxeznT27dgRI"
   },
   "outputs": [],
   "source": [
    "def plot(images):\n",
    "  #enter a square num of images\n",
    "  num = np.sqrt(len(images))\n",
    "  plt.figure(figsize=(num,num))\n",
    "  for index, image in enumerate(images):\n",
    "    plt.subplot(num,num,index+1)\n",
    "    plt.imshow(image[:,:,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjG2S24cc2xF",
    "outputId": "0ebd021e-a97b-45cc-86c8-b9e03789b493"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smwTa5HEv75j",
    "outputId": "97b00856-8003-41e1-b08e-325e975cdf7b"
   },
   "outputs": [],
   "source": [
    "gen.save(\"/content/drive/MyDrive/condGAN_FashionMNIST/gen\")\n",
    "dis.save(\"/content/drive/MyDrive/condGAN_FashionMNIST/dis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IaRH4qXwTDM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "condGAN_FashionMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
